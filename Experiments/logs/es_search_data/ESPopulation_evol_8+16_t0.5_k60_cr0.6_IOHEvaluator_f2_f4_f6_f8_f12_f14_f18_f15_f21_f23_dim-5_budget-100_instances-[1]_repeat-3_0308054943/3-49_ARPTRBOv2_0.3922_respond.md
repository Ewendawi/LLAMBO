# Description
**ARPTRBOv2: Adaptive Regularized Pareto Trust Region Bayesian Optimization with Enhanced Agreement and Diversity** This algorithm refines ARPTRBO by improving the model agreement check and incorporating a diversity metric directly into the Pareto selection process. It uses a Gaussian Process Regression (GPR) model with a Mat√©rn kernel for surrogate modeling within an adaptive trust region. The acquisition function is a Pareto front of Expected Improvement (EI), Probability of Improvement (PI), Upper Confidence Bound (UCB), and a diversity term based on the minimum distance to existing points. The trust region size is adaptively adjusted based on a more robust model agreement check that considers the uncertainty of the GPR model.

# Justification
1.  **Enhanced Model Agreement Check:** The original model agreement check used a simple correlation coefficient. This is replaced with a more robust check that considers the uncertainty (sigma) predicted by the Gaussian Process. The agreement is now based on the normalized root mean squared error (NRMSE), penalizing large errors relative to the predicted uncertainty. This provides a more reliable indicator of model fidelity.
2.  **Diversity Incorporation:** The original algorithm added random points when the Pareto front had fewer points than the batch size. This is improved by directly incorporating a diversity term into the Pareto selection. The diversity term is the minimum Euclidean distance from a candidate point to all previously evaluated points. This encourages exploration of less-explored regions of the search space.
3.  **Adaptive Regularization and Exploration Factor:** The adaptive regularization and exploration factor are retained as they proved beneficial in the original ARPTRBO.
4.  **Pareto Efficient Selection:** The Pareto efficient selection is retained as it is a good way to manage multiple acquisition functions.
5.  **Trust Region Adaptation:** The trust region adaptation is retained as it is a good way to balance exploration and exploitation.

# Code
```python
from collections.abc import Callable
from scipy.stats import qmc
from scipy.stats import norm
import numpy as np
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import Matern, ConstantKernel as C
from scipy.optimize import minimize
from sklearn.metrics import mean_squared_error

class ARPTRBOv2:
    def __init__(self, budget:int, dim:int):
        self.budget = budget
        self.dim = dim
        # bounds has shape (2,<dimension>), bounds[0]: lower bound, bounds[1]: upper bound
        self.bounds = np.array([[-5.0]*dim, [5.0]*dim])
        # X has shape (n_points, n_dims), y has shape (n_points, 1)
        self.X: np.ndarray = None
        self.y: np.ndarray = None
        self.n_evals = 0 # the number of function evaluations
        self.n_init = 2 * dim
        self.trust_region_radius = 2.0  # Initial trust region radius
        self.trust_region_shrink_factor = 0.5
        self.trust_region_expand_factor = 2.0
        self.model_agreement_threshold = 0.75
        self.best_x = None
        self.best_y = float('inf')
        self.acquisition_functions = ['ei', 'pi', 'ucb']
        self.ucb_kappa = 2.0 # Initial value
        self.ucb_kappa_max = 5.0
        self.ucb_kappa_min = 1.0
        self.reg_weight = 0.1 # Initial weight for the regularization term
        self.exploration_factor = 0.01 # Add an exploration factor

        # Do not add any other arguments without a default value

    def _sample_points(self, n_points):
        # sample points
        # return array of shape (n_points, n_dims)
        sampler = qmc.LatinHypercube(d=self.dim)
        sample = sampler.random(n=n_points)
        return qmc.scale(sample, self.bounds[0], self.bounds[1])

    def _fit_model(self, X, y):
        # Fit and tune surrogate model
        # return the model
        # Do not change the function signature
        kernel = C(1.0, (1e-3, 1e3)) * Matern(length_scale=1.0, length_scale_bounds=(1e-2, 1e2), nu=1.5)
        model = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=5, alpha=1e-5)
        model.fit(X, y)
        return model

    def _acquisition_function(self, X, model, iteration, acq_type='ei'):
        # Implement acquisition function
        # calculate the acquisition function value for each point in X
        # return array of shape (n_points, 1)
        mu, sigma = model.predict(X, return_std=True)
        mu = mu.reshape(-1, 1)
        sigma = sigma.reshape(-1, 1)

        if self.best_y is None:
            ei = np.zeros_like(mu)
        else:
            imp = self.best_y - mu
            Z = imp / sigma

            if acq_type == 'ei':
                ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)
                ei[sigma <= 1e-6] = 0.0  # handle zero sigma
            elif acq_type == 'pi':
                ei = norm.cdf(Z)
            elif acq_type == 'ucb':
                ei = mu + self.ucb_kappa * sigma
            else:
                raise ValueError("Invalid acquisition function type.")

        # Adaptive Regularization
        reg_weight = self.reg_weight * (1 - (iteration / self.budget))
        regularization_term = -reg_weight * np.linalg.norm(mu / (sigma + 1e-6), axis=1, keepdims=True)**2 # Uncertainty aware regularization
        ei = ei + regularization_term + self.exploration_factor * sigma # Add exploration factor

        return ei

    def _is_pareto_efficient(self, points):
        """
        Find the pareto-efficient points
        :param points: An n by m matrix of points
        :return: A boolean array of length n, True for pareto-efficient points, False otherwise
        """
        is_efficient = np.ones(points.shape[0], dtype=bool)
        for i, c in enumerate(points):
            if is_efficient[i]:
                is_efficient[is_efficient] = np.any(points[is_efficient] > c, axis=1) | (points[is_efficient] == c).all(axis=1)
                is_efficient[i] = True  # Keep current point
        return is_efficient

    def _calculate_diversity(self, X):
        """
        Calculates the minimum Euclidean distance from each point in X to all previously evaluated points.
        """
        if self.X is None or len(self.X) == 0:
            return np.zeros(X.shape[0])  # No diversity if no points have been evaluated

        distances = np.zeros(X.shape[0])
        for i in range(X.shape[0]):
            distances[i] = np.min(np.linalg.norm(X[i] - self.X, axis=1))
        return distances

    def _select_next_points(self, batch_size, model, iteration, trust_region_center):
        # Select the next points to evaluate
        # Use a selection strategy to optimize/leverage the acquisition function
        # The selection strategy can be any heuristic/evolutionary/mathematical/hybrid methods.
        # return array of shape (batch_size, n_dims)

        # Sample within the trust region using Sobol sequence
        sobol_engine = qmc.Sobol(d=self.dim, seed=42)
        samples = sobol_engine.random(n=100 * batch_size)

        # Scale and shift samples to be within the trust region
        scaled_samples = trust_region_center + self.trust_region_radius * (2 * samples - 1)

        # Clip samples to stay within the problem bounds
        scaled_samples = np.clip(scaled_samples, self.bounds[0], self.bounds[1])

        acquisition_values = np.zeros((scaled_samples.shape[0], len(self.acquisition_functions) + 1)) # +1 for diversity

        for i, acq_type in enumerate(self.acquisition_functions):
            acquisition_values[:, i] = self._acquisition_function(scaled_samples, model, iteration, acq_type).flatten()

        # Calculate diversity
        diversity = self._calculate_diversity(scaled_samples)
        acquisition_values[:, -1] = diversity # Add diversity to the acquisition values

        # Find Pareto front
        is_efficient = self._is_pareto_efficient(acquisition_values)
        pareto_points = scaled_samples[is_efficient]

        # Select top batch_size points from Pareto front
        if len(pareto_points) > batch_size:
            # Randomly select if more than batch_size
            indices = np.random.choice(len(pareto_points), batch_size, replace=False)
            selected_points = pareto_points[indices]
        else:
            # Use all Pareto points if less than or equal to batch_size
            selected_points = pareto_points

            # If still less than batch_size, sample randomly
            if len(selected_points) < batch_size:
                remaining = batch_size - len(selected_points)
                random_points = self._sample_points(remaining)
                selected_points = np.vstack((selected_points, random_points))

        return selected_points

    def _evaluate_points(self, func, X):
        # Evaluate the points in X
        # func: takes array of shape (n_dims,) and returns np.float64.
        # return array of shape (n_points, 1)
        y = np.array([func(x) for x in X]).reshape(-1, 1)
        self.n_evals += len(X)
        return y

    def _update_eval_points(self, new_X, new_y):
        # Update self.X and self.y
        # Do not change the function signature
        if self.X is None:
            self.X = new_X
            self.y = new_y
        else:
            self.X = np.vstack((self.X, new_X))
            self.y = np.vstack((self.y, new_y))

        # Update best seen value
        idx = np.argmin(self.y)
        if self.y[idx][0] < self.best_y:
            self.best_y = self.y[idx][0]
            self.best_x = self.X[idx]

    def __call__(self, func:Callable[[np.ndarray], np.float64]) -> tuple[np.float64, np.array]:
        # Main minimize optimization loop
        # func: takes array of shape (n_dims,) and returns np.float64.
        # !!! Do not call func directly. Use _evaluate_points instead and be aware of the budget when calling it. !!!
        # Return a tuple (best_y, best_x)

        # Initial sampling
        initial_X = self._sample_points(self.n_init)
        initial_y = self._evaluate_points(func, initial_X)
        self._update_eval_points(initial_X, initial_y)

        trust_region_center = self.X[np.argmin(self.y)] # Initialize trust region center

        iteration = self.n_init
        while self.n_evals < self.budget:
            # Adaptive batch size
            batch_size = max(1, int(5 * (1 - self.n_evals / self.budget))) # Linearly decreasing batch size

            # Optimization
            model = self._fit_model(self.X, self.y)

            # select points by acquisition function
            next_X = self._select_next_points(batch_size, model, iteration, trust_region_center)
            next_y = self._evaluate_points(func, next_X)
            self._update_eval_points(next_X, next_y)

            # Model agreement check (using NRMSE)
            predicted_y, sigma = model.predict(next_X, return_std=True)
            predicted_y = predicted_y.reshape(-1, 1)
            sigma = sigma.reshape(-1, 1)
            
            # Normalize by uncertainty
            normalized_error = (next_y - predicted_y) / (sigma + 1e-6)
            
            # Calculate NRMSE
            agreement = 1.0 / (1.0 + np.sqrt(np.mean(normalized_error**2))) # A higher value means better agreement

            # Adjust trust region size
            if np.isnan(agreement) or agreement < self.model_agreement_threshold:
                self.trust_region_radius *= self.trust_region_shrink_factor
                self.ucb_kappa = min(self.ucb_kappa * 1.1, self.ucb_kappa_max) # Increase kappa for exploration
            else:
                self.trust_region_radius *= self.trust_region_expand_factor
                self.trust_region_radius = min(self.trust_region_radius, 5.0) # Limit expansion
                self.ucb_kappa = max(self.ucb_kappa * 0.9, self.ucb_kappa_min) # Decrease kappa for exploitation

            # Update trust region center
            trust_region_center = self.X[np.argmin(self.y)]
            iteration += batch_size

        return self.best_y, self.best_x
```
## Feedback
 The algorithm ARPTRBOv2 got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.1687 with standard deviation 0.1120.

took 347.06 seconds to run.