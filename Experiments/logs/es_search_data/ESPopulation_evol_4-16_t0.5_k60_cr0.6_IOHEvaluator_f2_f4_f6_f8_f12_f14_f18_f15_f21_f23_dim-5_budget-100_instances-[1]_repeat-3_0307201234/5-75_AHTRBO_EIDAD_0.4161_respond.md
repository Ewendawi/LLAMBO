# Description
**Adaptive Hybrid Trust Region Bayesian Optimization with Dynamic Batch Size, EI, and Adaptive Diversity (AHTRBO-EIDAD):** This algorithm builds upon AHVTRBO_EI_DBS and HybridTrustRegionBO_EIV by introducing an adaptive diversity mechanism within the Expected Improvement (EI) acquisition function. Instead of a fixed `diversity_weight`, it dynamically adjusts this weight based on the current trust region size and the agreement between the Gaussian Process (GP) and Gradient Boosting (GB) models. This allows for more aggressive exploration when the trust region is large or model disagreement is high, and more focused exploitation when the trust region is small and models agree. This also introduces a mechanism to restart the optimization from a new random location if the trust region shrinks below a certain threshold, indicating potential stagnation.

# Justification
The core idea is to improve the balance between exploration and exploitation.

*   **Adaptive Diversity Weight:** The `diversity_weight` is crucial for volume-aware exploration. By making it adaptive, the algorithm can respond to the search landscape more effectively. When the trust region is large, exploration is encouraged. When the models disagree, exploration is also encouraged to resolve the uncertainty. When the trust region is small and models agree, the algorithm exploits the current promising region.
*   **Trust Region Restart:** If the trust region shrinks too much, it indicates that the algorithm might be stuck in a local optimum. Restarting from a new random location helps to escape such situations and explore new regions of the search space.
*   **Hybrid Surrogate Model (GP & GB):** This combines the advantages of both GP (good uncertainty quantification) and GB (better handling of high-dimensional data and complex relationships).
*   **Dynamic Batch Size:** Adapting the batch size to the remaining budget allows for efficient use of function evaluations, particularly towards the end of the optimization process.
*   **Adaptive GP Weight:** Adjusting the weight given to the GP model based on its error relative to the GB model allows the algorithm to dynamically favor the more accurate surrogate model.

# Code
```python
from collections.abc import Callable
from scipy.stats import qmc
import numpy as np
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import Matern, ConstantKernel
from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemble import HistGradientBoostingRegressor
from scipy.optimize import minimize
from sklearn.impute import SimpleImputer
from scipy.stats import norm
from scipy.spatial.distance import cdist
from sklearn.neighbors import NearestNeighbors


class AHTRBO_EIDAD:
    def __init__(self, budget: int, dim: int):
        self.budget = budget
        self.dim = dim
        self.bounds = np.array([[-5.0] * dim, [5.0] * dim])
        self.X: np.ndarray = None
        self.y: np.ndarray = None
        self.n_evals = 0
        self.n_init = 2 * self.dim
        self.trust_region_size = 2.0
        self.exploration_factor = 1.0
        self.diversity_weight = 0.01  # Initial value
        self.imputer = SimpleImputer(strategy='mean')
        self.epsilon = 1e-6
        self.gp_weight = 0.5  # Initial weight for GP model
        self.batch_size = 1
        self.knn = NearestNeighbors(n_neighbors=5, algorithm='kd_tree')
        self.min_trust_region_size = 0.1

        # Do not add any other arguments without a default value

    def _sample_points(self, n_points):
        sampler = qmc.Sobol(d=self.dim, scramble=False)
        samples = sampler.random(n=n_points)
        return qmc.scale(samples, self.bounds[0], self.bounds[1])

    def _fit_gp_model(self, X, y):
        kernel = ConstantKernel(1.0, constant_value_bounds="fixed") * Matern(length_scale=1.0, nu=2.5)
        model = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=5, alpha=1e-5)
        model.fit(X, y)
        return model

    def _fit_gb_model(self, X, y):
        # Impute missing values if any
        if np.isnan(X).any() or np.isnan(y).any():
            X = self.imputer.fit_transform(X)
            y = self.imputer.fit_transform(y)

        model = HistGradientBoostingRegressor(random_state=0)
        model.fit(X, y.ravel())
        return model

    def _expected_improvement(self, X, best_y):
        mu_gp, sigma = self.gp_model.predict(X, return_std=True)
        mu_gp = mu_gp.reshape(-1, 1)
        sigma = sigma.reshape(-1, 1)
        mu_gb = self.gb_model.predict(X).reshape(-1, 1)

        # Weighted average of GP and GB predictions
        mu = self.gp_weight * mu_gp + (1 - self.gp_weight) * mu_gb
        sigma = np.maximum(sigma, 1e-6) # Prevent division by zero

        imp = best_y - mu
        z = imp / sigma
        ei = imp * norm.cdf(z) + sigma * norm.pdf(z)

        # Volume-aware exploration with adaptive diversity weight
        distances, _ = self.knn.kneighbors(X)
        avg_distances = np.mean(distances, axis=1).reshape(-1, 1)
        ei += self.diversity_weight * self.exploration_factor * avg_distances

        return ei

    def _select_next_points(self, batch_size):
        best_idx = np.argmin(self.y)
        best_y = self.y[best_idx][0]
        best_x = self.X[best_idx]

        x_starts = best_x + np.random.normal(0, 0.1, size=(batch_size, self.dim))
        x_starts = np.clip(x_starts, self.bounds[0], self.bounds[1])

        candidates = []
        values = []
        for x_start in x_starts:
            lower_bound = np.maximum(x_start - self.trust_region_size / 2, self.bounds[0])
            upper_bound = np.minimum(x_start + self.trust_region_size / 2, self.bounds[1])

            res = minimize(lambda x: -self._expected_improvement(x.reshape(1, -1), best_y),
                           x_start,
                           bounds=np.array([lower_bound, upper_bound]).T,
                           method="L-BFGS-B")
            candidates.append(res.x)
            values.append(-res.fun)

        return np.array(candidates)

    def _evaluate_points(self, func, X):
        y = np.array([func(x) for x in X]).reshape(-1, 1)
        self.n_evals += len(X)
        return y

    def _update_eval_points(self, new_X, new_y):
        if self.X is None:
            self.X = new_X
            self.y = new_y
        else:
            self.X = np.vstack((self.X, new_X))
            self.y = np.vstack((self.y, new_y))
        self.knn.fit(self.X)

    def __call__(self, func: Callable[[np.ndarray], np.float64]) -> tuple[np.float64, np.array]:
        X_init = self._sample_points(self.n_init)
        y_init = self._evaluate_points(func, X_init)
        self._update_eval_points(X_init, y_init)

        self.gp_model = self._fit_gp_model(self.X, self.y)
        self.gb_model = self._fit_gb_model(self.X, self.y)

        while self.n_evals < self.budget:
            # Dynamic batch size adjustment
            self.batch_size = int(np.ceil((self.budget - self.n_evals) / 50.0))
            self.batch_size = max(1, min(self.batch_size, 10))  # Limit batch size

            X_next = self._select_next_points(self.batch_size)
            y_next = self._evaluate_points(func, X_next)
            self._update_eval_points(X_next, y_next)

            # Adaptive trust region adjustment
            mu_gp, sigma = self.gp_model.predict(X_next, return_std=True)
            mu_gp = mu_gp.reshape(-1, 1)
            mu_gb = self.gb_model.predict(X_next).reshape(-1, 1)
            y_pred = self.gp_weight * mu_gp + (1 - self.gp_weight) * mu_gb

            agreement = np.abs(y_pred - y_next)
            mean_agreement = np.mean(agreement)

            if mean_agreement < 1.0:
                self.trust_region_size *= 1.1
            else:
                self.trust_region_size *= 0.9

            self.trust_region_size = np.clip(self.trust_region_size, 0.1, 5.0)

            # Trust region restart mechanism
            if self.trust_region_size < self.min_trust_region_size:
                X_init = self._sample_points(self.n_init)
                y_init = self._evaluate_points(func, X_init)
                self._update_eval_points(X_init, y_init)
                self.trust_region_size = 2.0  # Reset trust region size
                self.gp_model = self._fit_gp_model(self.X, self.y)
                self.gb_model = self._fit_gb_model(self.X, self.y)
                continue # Skip the rest of the loop and restart

            # Dynamic exploration factor adjustment
            self.exploration_factor = 0.5 + (self.budget - self.n_evals) / self.budget

            # Adaptive GP weight adjustment
            gp_error = np.mean(np.abs(mu_gp - y_next))
            gb_error = np.mean(np.abs(mu_gb - y_next))

            if gp_error < gb_error:
                self.gp_weight = min(1.0, self.gp_weight + 0.05)
            else:
                self.gp_weight = max(0.0, self.gp_weight - 0.05)

            # Adaptive diversity weight adjustment
            self.diversity_weight = 0.01 + 0.1 * (self.trust_region_size / 5.0) + 0.1 * (1.0 if mean_agreement > 1.0 else 0.0)
            self.diversity_weight = np.clip(self.diversity_weight, 0.01, 0.1)  # Keep it within reasonable bounds

            self.gp_model = self._fit_gp_model(self.X, self.y)
            self.gb_model = self._fit_gb_model(self.X, self.y)

        best_idx = np.argmin(self.y)
        best_y = self.y[best_idx][0]
        best_x = self.X[best_idx]

        return best_y, best_x
```
## Feedback
 The algorithm AHTRBO_EIDAD got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.1880 with standard deviation 0.0976.

took 896.15 seconds to run.