# Description
**AHTRBO_EI_DBS_Improved:** This algorithm enhances the AHTRBO-EI-DBS by incorporating several key improvements: 1) Adaptive Learning Rate for Trust Region: Instead of fixed multiplicative updates, the trust region size is adjusted based on the prediction error of the hybrid model, using a learning rate that decays over time. 2) Enhanced Diversity Term: The diversity term in the EI acquisition function is modified to consider the average distance to the k-nearest neighbors instead of just the minimum distance, promoting a more balanced exploration. 3) Improved Batch Size Adjustment: The batch size is adjusted based on the remaining budget and the uncertainty (sigma) predicted by the GP model, leading to more informed batch size selection. 4) Noise handling in GP model: The alpha parameter in GP model is dynamically adjusted based on the observed variance in the evaluations, allowing the GP to handle noisy functions more effectively.

# Justification
The improvements address some limitations of the original AHTRBO-EI-DBS. The adaptive learning rate for the trust region allows for finer control and prevents overshooting or undershooting the optimal trust region size. The enhanced diversity term encourages exploration of diverse regions, mitigating premature convergence. Adjusting the batch size based on uncertainty allows for more efficient sampling, focusing on regions with higher uncertainty when the budget allows. The dynamic noise handling in GP model improves the robustness of the algorithm in the presence of noise. These changes aim to improve the exploration-exploitation balance and overall performance of the algorithm.

# Code
```python
from collections.abc import Callable
from scipy.stats import qmc
import numpy as np
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import Matern, ConstantKernel
from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemble import HistGradientBoostingRegressor
from scipy.optimize import minimize
from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances
from sklearn.impute import SimpleImputer
from scipy.stats import norm
from scipy.spatial.distance import cdist
from sklearn.neighbors import NearestNeighbors


class AHTRBO_EI_DBS_Improved:
    def __init__(self, budget: int, dim: int):
        self.budget = budget
        self.dim = dim
        self.bounds = np.array([[-5.0] * dim, [5.0] * dim])
        self.X: np.ndarray = None
        self.y: np.ndarray = None
        self.n_evals = 0
        self.n_init = 2 * self.dim
        self.trust_region_size = 2.0
        self.exploration_factor = 1.0
        self.diversity_weight = 0.01
        self.imputer = SimpleImputer(strategy='mean')
        self.epsilon = 1e-6
        self.gp_weight = 0.5  # Initial weight for GP model
        self.batch_size = 1
        self.trust_region_lr = 0.1  # Learning rate for trust region
        self.gp_alpha = 1e-5  # Initial alpha for GP model

        # Do not add any other arguments without a default value

    def _sample_points(self, n_points):
        sampler = qmc.Sobol(d=self.dim, scramble=False)
        samples = sampler.random(n=n_points)
        return qmc.scale(samples, self.bounds[0], self.bounds[1])

    def _fit_gp_model(self, X, y):
        kernel = ConstantKernel(1.0, constant_value_bounds="fixed") * Matern(length_scale=1.0, nu=2.5)
        model = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=5, alpha=self.gp_alpha)
        model.fit(X, y)
        return model

    def _fit_gb_model(self, X, y):
        # Impute missing values if any
        if np.isnan(X).any() or np.isnan(y).any():
            X = self.imputer.fit_transform(X)
            y = self.imputer.fit_transform(y)

        model = HistGradientBoostingRegressor(random_state=0)
        model.fit(X, y.ravel())
        return model

    def _expected_improvement(self, X, best_y):
        mu_gp, sigma = self.gp_model.predict(X, return_std=True)
        mu_gp = mu_gp.reshape(-1, 1)
        sigma = sigma.reshape(-1, 1)
        mu_gb = self.gb_model.predict(X).reshape(-1, 1)

        # Weighted average of GP and GB predictions
        mu = self.gp_weight * mu_gp + (1 - self.gp_weight) * mu_gb
        sigma = np.maximum(sigma, 1e-6) # Prevent division by zero

        imp = best_y - mu
        z = imp / sigma
        ei = imp * norm.cdf(z) + sigma * norm.pdf(z)

        # Enhanced Diversity term using k-Nearest Neighbors
        if self.X is not None and len(self.X) > 5:
            knn = NearestNeighbors(n_neighbors=min(5, len(self.X))).fit(self.X)
            distances, _ = knn.kneighbors(X)
            avg_distances = np.mean(distances, axis=1).reshape(-1, 1)
            ei += self.diversity_weight * self.exploration_factor * avg_distances
        return ei

    def _select_next_points(self, batch_size):
        best_idx = np.argmin(self.y)
        best_y = self.y[best_idx][0]
        best_x = self.X[best_idx]

        x_starts = best_x + np.random.normal(0, 0.1, size=(batch_size, self.dim))
        x_starts = np.clip(x_starts, self.bounds[0], self.bounds[1])

        candidates = []
        values = []
        for x_start in x_starts:
            lower_bound = np.maximum(x_start - self.trust_region_size / 2, self.bounds[0])
            upper_bound = np.minimum(x_start + self.trust_region_size / 2, self.bounds[1])

            res = minimize(lambda x: -self._expected_improvement(x.reshape(1, -1), best_y),
                           x_start,
                           bounds=np.array([lower_bound, upper_bound]).T,
                           method="L-BFGS-B")
            candidates.append(res.x)
            values.append(-res.fun)

        return np.array(candidates)

    def _evaluate_points(self, func, X):
        y = np.array([func(x) for x in X]).reshape(-1, 1)
        self.n_evals += len(X)
        return y

    def _update_eval_points(self, new_X, new_y):
        if self.X is None:
            self.X = new_X
            self.y = new_y
        else:
            self.X = np.vstack((self.X, new_X))
            self.y = np.vstack((self.y, new_y))

    def __call__(self, func: Callable[[np.ndarray], np.float64]) -> tuple[np.float64, np.array]:
        X_init = self._sample_points(self.n_init)
        y_init = self._evaluate_points(func, X_init)
        self._update_eval_points(X_init, y_init)

        self.gp_model = self._fit_gp_model(self.X, self.y)
        self.gb_model = self._fit_gb_model(self.X, self.y)

        while self.n_evals < self.budget:
            # Dynamic batch size adjustment based on uncertainty
            _, sigma = self.gp_model.predict(self.X, return_std=True)
            avg_sigma = np.mean(sigma)
            self.batch_size = int(np.ceil((self.budget - self.n_evals) * (avg_sigma / np.std(self.y))))
            self.batch_size = max(1, min(self.batch_size, 10))  # Limit batch size

            X_next = self._select_next_points(self.batch_size)
            y_next = self._evaluate_points(func, X_next)
            self._update_eval_points(X_next, y_next)

            # Adaptive trust region adjustment with learning rate
            mu_gp, _ = self.gp_model.predict(X_next, return_std=True)
            mu_gp = mu_gp.reshape(-1, 1)
            mu_gb = self.gb_model.predict(X_next).reshape(-1, 1)
            y_pred = self.gp_weight * mu_gp + (1 - self.gp_weight) * mu_gb

            agreement = np.abs(y_pred - y_next)
            mean_agreement = np.mean(agreement)

            if mean_agreement < 1.0:
                self.trust_region_size *= (1 + self.trust_region_lr)
            else:
                self.trust_region_size *= (1 - self.trust_region_lr)

            self.trust_region_size = np.clip(self.trust_region_size, 0.1, 5.0)
            self.trust_region_lr *= 0.95  # Decay learning rate

            # Dynamic exploration factor adjustment
            self.exploration_factor = 0.5 + (self.budget - self.n_evals) / self.budget

            # Adaptive GP weight adjustment
            gp_error = np.mean(np.abs(mu_gp - y_next))
            gb_error = np.mean(np.abs(mu_gb - y_next))

            if gp_error < gb_error:
                self.gp_weight = min(1.0, self.gp_weight + 0.05)
            else:
                self.gp_weight = max(0.0, self.gp_weight - 0.05)

            # Adjust GP alpha based on observed variance
            self.gp_alpha = np.var(self.y) * 1e-5

            self.gp_model = self._fit_gp_model(self.X, self.y)
            self.gb_model = self._fit_gb_model(self.X, self.y)

        best_idx = np.argmin(self.y)
        best_y = self.y[best_idx][0]
        best_x = self.X[best_idx]

        return best_y, best_x
```
## Feedback
 The algorithm AHTRBO_EI_DBS_Improved got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.1787 with standard deviation 0.0933.

took 1107.19 seconds to run.