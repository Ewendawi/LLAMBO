# Description
**AETSALSDE_KBA_MBO: Adaptive Ensemble with Thompson Sampling, Dynamic Exploration, Uncertainty-Aware Local Search with Momentum, Kernel Density Estimation with Bandwidth Adaptation, and Dynamic Ensemble Size Adjustment Bayesian Optimization.** This algorithm synergizes the strengths of AETSALSARBO and ABETSALSDE_KBA_BO, incorporating adaptive momentum in the local search, kernel density estimation (KDE) with adaptive bandwidth, and dynamic ensemble size adjustment. It uses an ensemble of Gaussian Process Regression (GPR) models with varying length scales and Thompson Sampling for efficient acquisition. The batch size is dynamically adjusted based on GPR uncertainty. The local search is enhanced with adaptive momentum, guided by GPR variance predictions, to refine solutions. The exploration strategy combines distance-based exploration with a dynamic exploration weight that considers both the optimization progress and the GPR uncertainty. KDE is incorporated for density-aware exploration, with the bandwidth dynamically adjusted based on nearest neighbor distances and optimization progress. The ensemble size is also dynamically adjusted based on optimization progress.

# Justification
This algorithm builds upon the strengths of AETSALSARBO and ABETSALSDE_KBA_BO, aiming to improve both exploration and exploitation.

*   **Ensemble of GPR Models:** Using an ensemble of GPR models with varying length scales allows the algorithm to capture different aspects of the function landscape, improving robustness.
*   **Thompson Sampling:** Thompson Sampling provides an efficient way to balance exploration and exploitation within the ensemble.
*   **Adaptive Batch Size:** Adjusting the batch size based on GPR uncertainty allows for more efficient use of function evaluations, focusing on exploration when uncertainty is high and exploitation when uncertainty is low.
*   **Uncertainty-Aware Local Search with Momentum:** Incorporating momentum into the local search helps to escape local optima and accelerate convergence, while the uncertainty-aware step size adjusts the search aggressiveness based on the GPR's variance predictions.
*   **Kernel Density Estimation (KDE) with Adaptive Bandwidth:** KDE helps to focus the search on high-density regions of promising solutions, while the adaptive bandwidth adjusts the KDE's sensitivity based on the local density of evaluated points and the optimization progress.
*   **Dynamic Exploration Weight:** Adjusting the exploration weight based on both the optimization progress and the GPR model uncertainty allows for a robust and efficient exploration-exploitation trade-off.
*   **Dynamic Ensemble Size:** Adjusting the ensemble size based on the optimization progress reduces computational cost in later stages.
*   **Combination of Strengths:** By combining the strengths of AETSALSARBO and ABETSALSDE_KBA_BO, this algorithm aims to achieve a better balance between global exploration and local exploitation, leading to improved performance on the BBOB test suite.

# Code
```python
from collections.abc import Callable
from scipy.stats import qmc
from scipy.stats import norm
import numpy as np
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.neighbors import KernelDensity, NearestNeighbors
from scipy.optimize import minimize

class AETSALSDE_KBA_MBO:
    def __init__(self, budget: int, dim: int):
        self.budget = budget
        self.dim = dim
        self.bounds = np.array([[-5.0] * dim, [5.0] * dim])
        self.X: np.ndarray = None
        self.y: np.ndarray = None
        self.n_evals = 0
        self.n_init = 2 * dim
        self.k_neighbors = min(10, 2 * dim)
        self.best_y = np.inf
        self.best_x = None
        self.kde_bandwidth = 0.5
        self.max_batch_size = min(10, dim)  # Maximum batch size for selecting points
        self.min_batch_size = 1
        self.exploration_weight = 0.2  # Initial exploration weight
        self.exploration_weight_min = 0.01  # Minimum exploration weight
        self.uncertainty_threshold = 0.5  # Threshold for adjusting batch size
        self.initial_local_search_radius = 0.2
        self.local_search_radius = self.initial_local_search_radius
        self.local_search_radius_min = 0.01
        self.local_search_step_size_factor = 0.1
        self.momentum = 0.1  # Momentum for local search

        self.max_models = 5  # Maximum number of surrogate models in the ensemble
        self.min_models = 1  # Minimum number of surrogate models in the ensemble
        self.models = []
        for i in range(self.max_models):
            length_scale = 1.0 * (i + 1) / self.max_models
            kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale, (1e-2, 1e2))
            model = GaussianProcessRegressor(
                kernel=kernel, n_restarts_optimizer=5, alpha=1e-5
            )
            self.models.append(model)

        self.previous_step = np.zeros(dim) # Initialize previous step for momentum

    def _sample_points(self, n_points):
        sampler = qmc.LatinHypercube(d=self.dim)
        sample = sampler.random(n=n_points)
        return qmc.scale(sample, self.bounds[0], self.bounds[1])

    def _fit_model(self, X, y):
        # Adapt ensemble size
        n_models = max(
            self.min_models, int(self.max_models * (1 - self.n_evals / self.budget))
        )
        active_models = self.models[:n_models]
        for model in active_models:
            model.fit(X, y)
        return active_models

    def _acquisition_function(self, X, active_models):
        # Thompson Sampling
        sampled_values = np.zeros((X.shape[0], len(active_models)))
        sigmas = np.zeros((X.shape[0], len(active_models)))
        for i, model in enumerate(active_models):
            mu, sigma = model.predict(X, return_std=True)
            mu = mu.reshape(-1, 1)
            sigma = sigma.reshape(-1, 1)
            sampled_values[:, i] = np.random.normal(mu.flatten(), sigma.flatten())
            sigmas[:, i] = sigma.flatten()

        acquisition_ts = np.mean(sampled_values, axis=1, keepdims=True)
        acquisition_ts = acquisition_ts.reshape(-1, 1)

        # Hybrid acquisition function (EI + exploration + KDE)
        mu = np.mean([model.predict(X) for model in active_models], axis=0).reshape(
            -1, 1
        )
        sigma = np.mean(sigmas, axis=1).reshape(-1, 1)

        imp = self.best_y - mu
        Z = imp / sigma
        ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)
        ei[sigma <= 1e-6] = 0.0

        # Distance-based exploration term
        if self.X is not None and len(self.X) > 0:
            min_dist = np.min(
                np.linalg.norm(X[:, None, :] - self.X[None, :, :], axis=2),
                axis=1,
                keepdims=True,
            )
            exploration = min_dist / np.max(min_dist)
        else:
            exploration = np.ones(X.shape[0]).reshape(-1, 1)

        # KDE-based exploration term
        if self.X is not None and len(self.X) > self.dim + 1:
            bandwidth = self._adaptive_bandwidth()
            kde = KernelDensity(bandwidth=bandwidth).fit(self.X)
            kde_scores = kde.score_samples(X)
            kde_scores = np.exp(kde_scores).reshape(-1, 1)  # Convert to density
            kde_exploration = kde_scores / np.max(kde_scores)  # Normalize
        else:
            kde_exploration = np.zeros(X.shape[0]).reshape(-1, 1)

        # Dynamic weighting
        exploration_weight_dyn = np.clip(1.0 - self.n_evals / self.budget, 0.1, 1.0)
        kde_weight = 1.0 - exploration_weight_dyn

        # Hybrid acquisition function
        acquisition = (
            ei
            + self.exploration_weight * exploration
            + kde_weight * kde_exploration
        )
        return acquisition

    def _adaptive_bandwidth(self):
        if self.X is None or len(self.X) < self.k_neighbors:
            return self.kde_bandwidth

        nbrs = NearestNeighbors(
            n_neighbors=self.k_neighbors, algorithm="ball_tree"
        ).fit(self.X)
        distances, _ = nbrs.kneighbors(self.X)
        k_distances = distances[:, -1]
        bandwidth_nn = np.median(k_distances)

        # Adjust bandwidth based on optimization progress
        bandwidth_decay = np.clip(1.0 - self.n_evals / self.budget, 0.1, 1.0)
        bandwidth = bandwidth_nn * bandwidth_decay

        return bandwidth

    def _select_next_points(self, batch_size, active_models):
        candidate_points = self._sample_points(50 * batch_size)

        # Add points around the best solution (local search)
        if self.best_x is not None:
            local_points = np.random.normal(loc=self.best_x, scale=self.local_search_radius, size=(50 * batch_size, self.dim))
            local_points = np.clip(local_points, self.bounds[0], self.bounds[1])
            candidate_points = np.vstack((candidate_points, local_points))

        acquisition_values = self._acquisition_function(candidate_points, active_models)
        indices = np.argsort(acquisition_values.flatten())[-batch_size:]
        next_points = candidate_points[indices]

        # Adaptive Local search with Momentum
        for i in range(batch_size):
            def obj_func(x):
                x = x.reshape(1, -1)
                return np.mean([model.predict(x)[0] for model in active_models])

            x0 = next_points[i].copy()
            bounds = [(self.bounds[0][j], self.bounds[1][j]) for j in range(self.dim)]

            # Uncertainty-aware local search
            uncertainty = np.mean([model.predict(x0.reshape(1, -1), return_std=True)[1] for model in active_models])
            maxiter = int(5 + 10 * uncertainty)
            maxiter = min(maxiter, 20)

            # Adaptive step size
            step_size = self.local_search_step_size_factor * uncertainty * (1 - self.n_evals / self.budget)

            # Apply momentum
            step = step_size * np.random.randn(self.dim)
            x0 = x0 + step + self.momentum * self.previous_step
            x0 = np.clip(x0, self.bounds[0], self.bounds[1])  # Clip to bounds
            self.previous_step = step  # Update previous step

            options = {'maxiter': maxiter, 'ftol': 1e-4}
            res = minimize(obj_func, x0, method='L-BFGS-B', bounds=bounds, options=options)
            next_points[i] = res.x

        return next_points

    def _evaluate_points(self, func, X):
        y = np.array([func(x) for x in X])
        self.n_evals += len(X)
        return y.reshape(-1, 1)

    def _update_eval_points(self, new_X, new_y):
        if self.X is None:
            self.X = new_X
            self.y = new_y
        else:
            self.X = np.vstack((self.X, new_X))
            self.y = np.vstack((self.y, new_y))

        idx = np.argmin(self.y)
        if self.y[idx][0] < self.best_y:
            self.best_y = self.y[idx][0]
            self.best_x = self.X[idx]

    def __call__(self, func: Callable[[np.ndarray], np.float64]) -> tuple[
        np.float64, np.array
    ]:
        initial_X = self._sample_points(self.n_init)
        initial_y = self._evaluate_points(func, initial_X)
        self._update_eval_points(initial_X, initial_y)

        active_models = self._fit_model(self.X, self.y)

        while self.n_evals < self.budget:
            remaining_evals = self.budget - self.n_evals

            # Adjust batch size based on uncertainty
            sigmas = []
            for model in active_models:
                _, sigma = model.predict(self.X, return_std=True)
                sigmas.append(np.mean(sigma))
            avg_sigma = np.mean(sigmas)

            if avg_sigma > self.uncertainty_threshold:
                batch_size = self.max_batch_size
            else:
                batch_size = self.min_batch_size

            batch_size = min(batch_size, remaining_evals)  # Adjust batch size to budget

            next_X = self._select_next_points(batch_size, active_models)
            next_y = self._evaluate_points(func, next_X)
            self._update_eval_points(next_X, next_y)
            active_models = self._fit_model(self.X, self.y)

            # Update exploration weight
            self.exploration_weight = max(self.exploration_weight_min, self.exploration_weight * (1 - self.n_evals / self.budget) * (1 + avg_sigma))

            # Update local search radius
            self.local_search_radius = max(self.local_search_radius_min, self.initial_local_search_radius * (1 - self.n_evals / self.budget) * (1 + avg_sigma))

        return self.best_y, self.best_x
```
## Feedback
 The algorithm AETSALSDE_KBA_MBO got an average Area over the convergence curve (AOCC, 1.0 is the best) score of 0.1802 with standard deviation 0.1077.

took 826.74 seconds to run.